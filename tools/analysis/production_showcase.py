#!/usr/bin/env python3
"""
Production Showcase - Demonstra√ß√£o Completa do Sistema
Ferramenta para teste de produ√ß√£o com an√°lise completa, visualiza√ß√µes e relat√≥rios
"""

import sys
import os
import json
import random
import cv2
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import logging

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.feature_extractor import FeatureExtractor
from src.core.person_detector import PersonDetector
from src.core.exposure_analyzer import ExposureAnalyzer


def convert_numpy_types(obj):
    """Convert numpy types to Python types for JSON serialization"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

# Configure logging to be less verbose
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

class ProductionShowcase:
    """
    Sistema de demonstra√ß√£o completa para produ√ß√£o
    """
    
    def __init__(self, output_dir: str = "data/analysis_results/production_showcase"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize components
        self.feature_extractor = FeatureExtractor()
        self.person_detector = PersonDetector()
        self.exposure_analyzer = ExposureAnalyzer()
        
        # Results storage
        self.results = {}
        self.analysis_summary = {}
        
    def select_sample_images(self, input_dir: str, count: int = 10) -> List[Path]:
        """
        Seleciona imagens de amostra de forma inteligente
        """
        input_path = Path(input_dir)
        all_images = list(input_path.glob("*.JPG")) + list(input_path.glob("*.jpg"))
        
        if len(all_images) < count:
            print(f"‚ö†Ô∏è Apenas {len(all_images)} imagens dispon√≠veis (solicitado: {count})")
            return all_images
            
        # Sele√ß√£o estratificada: algumas do in√≠cio, meio e fim para variedade
        selected = []
        step = len(all_images) // count
        
        for i in range(count):
            idx = i * step + random.randint(0, min(step-1, len(all_images)-1-i*step))
            if idx < len(all_images):
                selected.append(all_images[idx])
        
        # Garantir exatamente 'count' imagens √∫nicas
        selected = list(set(selected))
        while len(selected) < count and len(selected) < len(all_images):
            remaining = [img for img in all_images if img not in selected]
            selected.append(random.choice(remaining))
            
        return selected[:count]
    
    def analyze_image_complete(self, image_path: Path) -> Dict[str, Any]:
        """
        An√°lise completa de uma imagem
        """
        print(f"üîç Analisando: {image_path.name}")
        
        try:
            # Extract all features
            features = self.feature_extractor.extract_features(str(image_path))
            
            # Load image for visualization
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"N√£o foi poss√≠vel carregar a imagem: {image_path}")
            
            # Person detection with detailed info
            person_results = self.person_detector.detect_persons_and_faces(image)
            
            # Exposure analysis
            exposure_result = self.exposure_analyzer.analyze_exposure(image)
            
            # Compile complete analysis
            analysis = {
                'filename': image_path.name,
                'path': str(image_path),
                'timestamp': datetime.now().isoformat(),
                'image_dimensions': {
                    'height': image.shape[0],
                    'width': image.shape[1],
                    'channels': image.shape[2]
                },
                'features': features,
                'person_detection': {
                    'persons_detected': len(person_results.get('persons', [])) if person_results else 0,
                    'faces_detected': len(person_results.get('faces', [])) if person_results else 0,
                    'persons': []
                },
                'exposure_analysis': exposure_result,
                'quality_assessment': self._assess_quality(features),
                'recommendations': self._generate_recommendations(features)
            }
            
            # Add detailed person info
            if person_results and person_results.get('persons'):
                for i, person in enumerate(person_results.get('persons', [])):
                    person_info = {
                        'person_id': i,
                        'bbox': person.bounding_box,
                        'confidence': person.confidence,
                        'landmarks_count': len(person.landmarks) if person.landmarks else 0,
                        'dominance_score': person.dominance_score
                    }
                    analysis['person_detection']['persons'].append(person_info)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Erro ao analisar {image_path.name}: {e}")
            return {
                'filename': image_path.name,
                'path': str(image_path),
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def _assess_quality(self, features: Dict) -> Dict[str, Any]:
        """
        Avalia√ß√£o de qualidade baseada nas features
        """
        sharpness = features.get('sharpness_laplacian', 0)
        exposure = features.get('exposure_level', 'unknown')
        person_count = features.get('person_count', 0)
        dominant_score = features.get('dominant_person_score', 0)
        
        # Quality scoring
        quality_score = 0.0
        quality_factors = []
        
        # Sharpness assessment
        if sharpness > 100:
            quality_score += 0.3
            quality_factors.append("sharp")
        elif sharpness > 50:
            quality_score += 0.2
            quality_factors.append("moderately_sharp")
        else:
            quality_factors.append("blurry")
            
        # Exposure assessment
        if exposure in ['adequate', 'bright']:
            quality_score += 0.3
            quality_factors.append("good_exposure")
        elif exposure in ['dark', 'extremely_bright']:
            quality_score += 0.1
            quality_factors.append("exposure_issues")
        else:
            quality_factors.append("poor_exposure")
            
        # Person detection assessment
        if person_count > 0:
            quality_score += 0.2
            quality_factors.append("persons_detected")
            if dominant_score > 0.3:
                quality_score += 0.2
                quality_factors.append("strong_dominant_person")
        else:
            quality_factors.append("no_persons")
            
        # Final rating
        if quality_score >= 0.8:
            rating = "excellent"
        elif quality_score >= 0.6:
            rating = "good"
        elif quality_score >= 0.4:
            rating = "fair"
        elif quality_score >= 0.2:
            rating = "poor"
        else:
            rating = "reject"
            
        return {
            'overall_score': round(quality_score, 3),
            'rating': rating,
            'quality_factors': quality_factors,
            'technical_metrics': {
                'sharpness_score': sharpness,
                'exposure_level': exposure,
                'person_count': person_count,
                'dominant_person_score': round(dominant_score, 3)
            }
        }
    
    def _generate_recommendations(self, features: Dict) -> List[str]:
        """
        Gera recomenda√ß√µes baseadas na an√°lise
        """
        recommendations = []
        
        sharpness = features.get('sharpness_laplacian', 0)
        exposure = features.get('exposure_level', 'unknown')
        person_count = features.get('person_count', 0)
        
        if sharpness < 50:
            recommendations.append("‚ö†Ô∏è Imagem com blur detectado - considerar rejei√ß√£o para uso profissional")
        elif sharpness > 150:
            recommendations.append("‚úÖ Excelente nitidez - √≥tima para impress√£o e uso profissional")
            
        if exposure == 'extremely_dark':
            recommendations.append("üåë Imagem muito escura - requerer√° p√≥s-processamento significativo")
        elif exposure == 'extremely_bright':
            recommendations.append("‚òÄÔ∏è Imagem superexposta - informa√ß√µes podem estar perdidas")
        elif exposure == 'adequate':
            recommendations.append("‚úÖ Exposi√ß√£o adequada - pronta para uso")
            
        if person_count == 0:
            recommendations.append("üë§ Nenhuma pessoa detectada - verificar se √© intencional")
        elif person_count > 3:
            recommendations.append("üë• M√∫ltiplas pessoas detectadas - verificar enquadramento")
            
        if not recommendations:
            recommendations.append("‚úÖ Imagem em boas condi√ß√µes gerais")
            
        return recommendations
    
    def create_annotated_visualization(self, image_path: Path, analysis: Dict) -> Optional[Path]:
        """
        Cria visualiza√ß√£o anotada da imagem
        """
        try:
            # Load image
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"N√£o foi poss√≠vel carregar: {image_path}")
            
            # Create a copy for annotation
            annotated = image.copy()
            
            # Get person detection info
            persons_info = analysis.get('person_detection', {}).get('persons', [])
            
            # Draw person bounding boxes
            for i, person in enumerate(persons_info):
                bbox = person.get('bbox', [])
                if len(bbox) == 4:
                    x, y, w, h = bbox
                    
                    # Draw bounding box
                    cv2.rectangle(annotated, (x, y), (x + w, y + h), (0, 255, 0), 2)
                    
                    # Draw person label
                    label = f"Person {i+1} (Score: {person.get('dominance_score', 0):.2f})"
                    cv2.putText(annotated, label, (x, y - 10), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # Add analysis info overlay
            overlay_info = [
                f"Arquivo: {analysis['filename']}",
                f"Rating: {analysis['quality_assessment']['rating'].upper()}",
                f"Nitidez: {analysis['quality_assessment']['technical_metrics']['sharpness_score']:.1f}",
                f"Exposi√ß√£o: {analysis['quality_assessment']['technical_metrics']['exposure_level']}",
                f"Pessoas: {analysis['quality_assessment']['technical_metrics']['person_count']}"
            ]
            
            # Draw info box
            box_height = 25 * len(overlay_info) + 20
            cv2.rectangle(annotated, (10, 10), (400, box_height), (0, 0, 0), -1)
            cv2.rectangle(annotated, (10, 10), (400, box_height), (255, 255, 255), 2)
            
            for i, info in enumerate(overlay_info):
                y_pos = 35 + i * 25
                cv2.putText(annotated, info, (20, y_pos), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            # Save annotated image
            output_path = self.output_dir / f"annotated_{analysis['filename']}"
            cv2.imwrite(str(output_path), annotated)
            
            return output_path
            
        except Exception as e:
            logger.error(f"Erro ao criar visualiza√ß√£o para {image_path.name}: {e}")
            return None
    
    def generate_natural_language_report(self, analysis: Dict) -> str:
        """
        Gera relat√≥rio em linguagem natural para uma imagem
        """
        filename = analysis['filename']
        quality = analysis['quality_assessment']
        features = analysis['features']
        persons = analysis['person_detection']
        exposure = analysis['exposure_analysis']
        
        report = f"\nüì∏ **AN√ÅLISE: {filename}**\n"
        report += "=" * 50 + "\n"
        
        # Overall assessment
        rating = quality['rating']
        score = quality['overall_score']
        
        rating_emoji = {
            'excellent': 'üåü',
            'good': '‚úÖ', 
            'fair': '‚öñÔ∏è',
            'poor': '‚ö†Ô∏è',
            'reject': '‚ùå'
        }
        
        report += f"{rating_emoji.get(rating, '‚ùì')} **CLASSIFICA√á√ÉO: {rating.upper()}** (Score: {score:.1f}/1.0)\n\n"
        
        # Technical analysis
        sharpness = quality['technical_metrics']['sharpness_score']
        exposure_level = quality['technical_metrics']['exposure_level']
        person_count = quality['technical_metrics']['person_count']
        
        report += "üîç **AN√ÅLISE T√âCNICA:**\n"
        report += f"  ‚Ä¢ Nitidez: {sharpness:.1f} - "
        if sharpness > 100:
            report += "Excelente qualidade de foco\n"
        elif sharpness > 50:
            report += "Foco adequado\n"
        else:
            report += "Imagem com blur significativo\n"
            
        report += f"  ‚Ä¢ Exposi√ß√£o: {exposure_level} - "
        exposure_desc = {
            'extremely_dark': 'Muito escura, requer corre√ß√£o',
            'dark': 'Levemente escura',
            'adequate': 'Exposi√ß√£o ideal',
            'bright': 'Levemente clara', 
            'extremely_bright': 'Superexposta, pode ter perdas'
        }
        report += exposure_desc.get(exposure_level, 'N√≠vel desconhecido') + "\n"
        
        # Person analysis
        report += f"  ‚Ä¢ Pessoas detectadas: {person_count}\n"
        
        if person_count > 0:
            dominant_score = quality['technical_metrics']['dominant_person_score']
            report += f"  ‚Ä¢ Score da pessoa dominante: {dominant_score:.2f}\n"
            
            if dominant_score > 0.4:
                report += "    ‚Üí Pessoa muito bem posicionada e focada\n"
            elif dominant_score > 0.2:
                report += "    ‚Üí Pessoa adequadamente posicionada\n"
            else:
                report += "    ‚Üí Pessoa pode estar mal enquadrada ou desfocada\n"
        
        # Detailed insights
        report += "\nüí° **INSIGHTS DETALHADOS:**\n"
        
        # Quality factors analysis
        factors = quality['quality_factors']
        positive_factors = [f for f in factors if f in ['sharp', 'good_exposure', 'persons_detected', 'strong_dominant_person']]
        negative_factors = [f for f in factors if f in ['blurry', 'poor_exposure', 'no_persons', 'exposure_issues']]
        
        if positive_factors:
            report += "  ‚úÖ Pontos fortes:\n"
            for factor in positive_factors:
                factor_desc = {
                    'sharp': 'Imagem n√≠tida e bem focada',
                    'good_exposure': 'Exposi√ß√£o bem balanceada', 
                    'persons_detected': 'Pessoas identificadas com sucesso',
                    'strong_dominant_person': 'Pessoa principal bem destacada'
                }
                report += f"    ‚Ä¢ {factor_desc.get(factor, factor)}\n"
        
        if negative_factors:
            report += "  ‚ö†Ô∏è Pontos de aten√ß√£o:\n"
            for factor in negative_factors:
                factor_desc = {
                    'blurry': 'Falta de nitidez pode comprometer qualidade',
                    'poor_exposure': 'Problemas de exposi√ß√£o requerem corre√ß√£o',
                    'no_persons': 'Nenhuma pessoa identificada na imagem',
                    'exposure_issues': 'Exposi√ß√£o n√£o ideal mas recuper√°vel'
                }
                report += f"    ‚Ä¢ {factor_desc.get(factor, factor)}\n"
        
        # Recommendations
        recommendations = analysis['recommendations']
        if recommendations:
            report += "\nüéØ **RECOMENDA√á√ïES:**\n"
            for rec in recommendations:
                report += f"  {rec}\n"
        
        # Final verdict
        report += "\nüìã **VEREDICTO FINAL:**\n"
        if rating == 'excellent':
            report += "  üåü Imagem de excelente qualidade, ideal para qualquer uso profissional.\n"
        elif rating == 'good':
            report += "  ‚úÖ Boa qualidade geral, adequada para a maioria dos usos.\n"
        elif rating == 'fair':
            report += "  ‚öñÔ∏è Qualidade aceit√°vel, mas pode requerer edi√ß√£o menor.\n"
        elif rating == 'poor':
            report += "  ‚ö†Ô∏è Qualidade comprometida, usar apenas se necess√°rio.\n"
        else:
            report += "  ‚ùå Qualidade inadequada, recomenda-se rejei√ß√£o.\n"
        
        return report
    
    def run_production_showcase(self, input_dir: str = "data/input", sample_count: int = 10):
        """
        Executa demonstra√ß√£o completa de produ√ß√£o
        """
        print("üöÄ INICIANDO DEMONSTRA√á√ÉO DE PRODU√á√ÉO")
        print("=" * 60)
        
        # Select sample images
        print(f"üìÇ Selecionando {sample_count} imagens de amostra...")
        selected_images = self.select_sample_images(input_dir, sample_count)
        print(f"‚úÖ {len(selected_images)} imagens selecionadas")
        
        # Process each image
        all_analyses = []
        all_reports = []
        
        print(f"\nüîç Processando {len(selected_images)} imagens...")
        for i, image_path in enumerate(selected_images, 1):
            print(f"\n[{i}/{len(selected_images)}] {image_path.name}")
            
            # Complete analysis
            analysis = self.analyze_image_complete(image_path)
            all_analyses.append(analysis)
            
            # Create visualization
            if 'error' not in analysis:
                viz_path = self.create_annotated_visualization(image_path, analysis)
                if viz_path:
                    analysis['visualization_path'] = str(viz_path)
                    print(f"  üìä Visualiza√ß√£o salva: {viz_path.name}")
                
                # Generate natural language report
                report = self.generate_natural_language_report(analysis)
                all_reports.append(report)
                print(f"  üìù Relat√≥rio gerado")
            else:
                print(f"  ‚ùå Erro na an√°lise: {analysis.get('error', 'Desconhecido')}")
        
        # JSON results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = self.output_dir / f"production_showcase_results_{timestamp}.json"
        
        # Convert numpy types before JSON serialization
        json_data = convert_numpy_types({
            'timestamp': datetime.now().isoformat(),
            'sample_count': len(selected_images),
            'successful_analyses': len([a for a in all_analyses if 'error' not in a]),
            'analyses': all_analyses,
            'summary_statistics': self._generate_summary_stats(all_analyses)
        })
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        # Natural language report
        report_path = self.output_dir / f"production_showcase_report_{timestamp}.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# üìä RELAT√ìRIO DE DEMONSTRA√á√ÉO - PHOTO CULLING SYSTEM\n")
            f.write(f"**Data:** {datetime.now().strftime('%d/%m/%Y √†s %H:%M:%S')}\n")
            f.write(f"**Amostras analisadas:** {len(selected_images)}\n\n")
            
            for report in all_reports:
                f.write(report + "\n")
            
            # Summary
            f.write(self._generate_executive_summary(all_analyses))
        
        print(f"\nüéâ DEMONSTRA√á√ÉO CONCLU√çDA!")
        print(f"üìÑ Resultados JSON: {json_path}")
        print(f"üìù Relat√≥rio completo: {report_path}")
        print(f"üìä Visualiza√ß√µes: {self.output_dir}")
        
        return {
            'json_path': json_path,
            'report_path': report_path,
            'analyses': all_analyses,
            'visualizations_dir': self.output_dir
        }
    
    def _generate_summary_stats(self, analyses: List[Dict]) -> Dict[str, Any]:
        """
        Gera estat√≠sticas resumidas
        """
        successful = [a for a in analyses if 'error' not in a]
        
        if not successful:
            return {'error': 'Nenhuma an√°lise bem-sucedida'}
        
        # Collect metrics
        ratings = [a['quality_assessment']['rating'] for a in successful]
        scores = [a['quality_assessment']['overall_score'] for a in successful]
        sharpness = [a['quality_assessment']['technical_metrics']['sharpness_score'] for a in successful]
        person_counts = [a['quality_assessment']['technical_metrics']['person_count'] for a in successful]
        
        # Rating distribution
        rating_dist = {}
        for rating in ['excellent', 'good', 'fair', 'poor', 'reject']:
            rating_dist[rating] = ratings.count(rating)
        
        return {
            'total_analyzed': len(successful),
            'rating_distribution': rating_dist,
            'average_quality_score': round(sum(scores) / len(scores), 3),
            'average_sharpness': round(sum(sharpness) / len(sharpness), 1),
            'average_persons_per_image': round(sum(person_counts) / len(person_counts), 1),
            'best_image': max(successful, key=lambda x: x['quality_assessment']['overall_score'])['filename'],
            'worst_image': min(successful, key=lambda x: x['quality_assessment']['overall_score'])['filename']
        }
    
    def _generate_executive_summary(self, analyses: List[Dict]) -> str:
        """
        Gera resumo executivo
        """
        summary_stats = self._generate_summary_stats(analyses)
        
        if 'error' in summary_stats:
            return "\n## ‚ùå ERRO NO RESUMO EXECUTIVO\nN√£o foi poss√≠vel gerar estat√≠sticas.\n"
        
        summary = "\n" + "="*80 + "\n"
        summary += "## üìà RESUMO EXECUTIVO DA DEMONSTRA√á√ÉO\n"
        summary += "="*80 + "\n\n"
        
        summary += f"**üìä ESTAT√çSTICAS GERAIS:**\n"
        summary += f"  ‚Ä¢ Total de imagens analisadas: {summary_stats['total_analyzed']}\n"
        summary += f"  ‚Ä¢ Score m√©dio de qualidade: {summary_stats['average_quality_score']:.1f}/1.0\n"
        summary += f"  ‚Ä¢ Nitidez m√©dia: {summary_stats['average_sharpness']:.1f}\n"
        summary += f"  ‚Ä¢ Pessoas por imagem (m√©dia): {summary_stats['average_persons_per_image']:.1f}\n\n"
        
        summary += f"**üèÜ DISTRIBUI√á√ÉO DE QUALIDADE:**\n"
        dist = summary_stats['rating_distribution']
        for rating, count in dist.items():
            emoji = {'excellent': 'üåü', 'good': '‚úÖ', 'fair': '‚öñÔ∏è', 'poor': '‚ö†Ô∏è', 'reject': '‚ùå'}
            if count > 0:
                pct = (count / summary_stats['total_analyzed']) * 100
                summary += f"  ‚Ä¢ {emoji.get(rating, '‚ùì')} {rating.capitalize()}: {count} imagens ({pct:.1f}%)\n"
        
        summary += f"\n**ü•á DESTAQUES:**\n"
        summary += f"  ‚Ä¢ Melhor imagem: {summary_stats['best_image']}\n"
        summary += f"  ‚Ä¢ Imagem com maior aten√ß√£o: {summary_stats['worst_image']}\n\n"
        
        summary += "**‚úÖ CONCLUS√ÉO:**\n"
        avg_score = summary_stats['average_quality_score']
        if avg_score >= 0.7:
            summary += "  üåü Excelente qualidade geral do dataset analisado!\n"
        elif avg_score >= 0.5:
            summary += "  ‚úÖ Boa qualidade geral com algumas oportunidades de melhoria.\n"
        elif avg_score >= 0.3:
            summary += "  ‚öñÔ∏è Qualidade mista, triagem recomendada.\n"
        else:
            summary += "  ‚ö†Ô∏è Dataset requer curadoria significativa.\n"
        
        summary += f"\n**üöÄ SISTEMA DE AN√ÅLISE:**\n"
        summary += f"  ‚Ä¢ 95 features autom√°ticas extra√≠das por imagem\n"
        summary += f"  ‚Ä¢ Detec√ß√£o de pessoas e an√°lise de qualidade\n"
        summary += f"  ‚Ä¢ Classifica√ß√£o autom√°tica e recomenda√ß√µes\n"
        summary += f"  ‚Ä¢ Visualiza√ß√µes com anota√ß√µes t√©cnicas\n"
        
        return summary


def main():
    """
    Execu√ß√£o principal da demonstra√ß√£o
    """
    showcase = ProductionShowcase()
    
    print("üöÄ PHOTO CULLING SYSTEM - DEMONSTRA√á√ÉO DE PRODU√á√ÉO")
    print("An√°lise completa com 10 imagens selecionadas")
    print("Gerando resultados JSON, visualiza√ß√µes e relat√≥rios...")
    
    results = showcase.run_production_showcase(
        input_dir="data/input",
        sample_count=10
    )
    
    print(f"\nüéØ ARQUIVOS GERADOS:")
    print(f"  üìÑ JSON: {results['json_path']}")
    print(f"  üìù Relat√≥rio: {results['report_path']}")
    print(f"  üìä Visualiza√ß√µes: {results['visualizations_dir']}")


if __name__ == "__main__":
    main()
