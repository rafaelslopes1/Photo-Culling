#!/usr/bin/env python3
"""
Visual Analysis Tester v2.5 - Photo Culling System
Comprehensive image analysis with visual detections and JSON debug output

This tool provides:
- Complete analysis of 50 sample images
- Visual overlay of detections (blur zones, faces, persons, exposure issues)
- Detailed JSON files for each image with all analysis data
- Summary report with statistics and insights
- Side-by-side comparison of original vs annotated images
"""

import os
import cv2
import json
import numpy as np
from datetime import datetime
from pathlib import Path
import sys
import logging
import random
from typing import Dict, List, Tuple, Optional
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.gridspec import GridSpec
import seaborn as sns

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import all analysis modules
from src.core.image_quality_analyzer import ImageQualityAnalyzer
from src.core.feature_extractor import FeatureExtractor
from src.core.unified_scoring_system import UnifiedScoringSystem
from src.core.person_detector import PersonDetector
from src.core.face_recognition_system import FaceRecognitionSystem
from src.core.exposure_analyzer import ExposureAnalyzer
from src.core.cropping_analyzer import CroppingAnalyzer

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VisualAnalysisTester:
    """
    Advanced visual analysis tester with comprehensive detection visualization
    and detailed JSON output for debugging and analysis.
    """
    
    def __init__(self, input_dir: str = "data/input", output_dir: str = "data/analysis_results"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        
        # Create output directories
        self.results_dir = self.output_dir / "visual_results"
        self.json_dir = self.output_dir / "json_debug"
        self.summary_dir = self.output_dir / "summary"
        
        for dir_path in [self.results_dir, self.json_dir, self.summary_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize analysis components
        self.quality_analyzer = ImageQualityAnalyzer()
        self.feature_extractor = FeatureExtractor()
        self.scoring_system = UnifiedScoringSystem()
        
        # Initialize advanced detection systems
        try:
            logger.info("ðŸš€ Inicializando detectores avanÃ§ados...")
            self.person_detector = PersonDetector()
            self.face_recognition = FaceRecognitionSystem()
            self.exposure_analyzer = ExposureAnalyzer()
            logger.info("âœ… Detectores avanÃ§ados inicializados com sucesso")
        except Exception as e:
            logger.warning(f"âš ï¸ Erro ao inicializar detectores avanÃ§ados: {e}")
            self.person_detector = None
            self.face_recognition = None
            self.exposure_analyzer = None
        
        # Load basic CV2 cascades for fallback detection
        try:
            import pkg_resources
            haar_path = '/usr/local/lib/python3.9/site-packages/cv2/data/'
            self.face_cascade = cv2.CascadeClassifier(haar_path + 'haarcascade_frontalface_default.xml')
            self.body_cascade = cv2.CascadeClassifier(haar_path + 'haarcascade_fullbody.xml')
        except:
            logger.warning("âš ï¸ NÃ£o foi possÃ­vel carregar classificadores Haar cascade")
            self.face_cascade = None
            self.body_cascade = None
        
        # Analysis results storage
        self.analysis_results = []
        self.statistics = {
            'total_images': 0,
            'successful_analysis': 0,
            'failed_analysis': 0,
            'blur_detected': 0,
            'faces_detected': 0,
            'persons_detected': 0,
            'exposure_issues': 0,
            'cropping_issues': 0,
            'score_distribution': {'excellent': 0, 'good': 0, 'acceptable': 0, 'poor': 0, 'rejected': 0}
        }
        
        # Visualization settings
        plt.style.use('seaborn-v0_8')
        self.colors = {
            'face': (0, 255, 0),        # Green for faces
            'person': (255, 0, 0),      # Red for person detection
            'blur_zone': (0, 0, 255),   # Blue for blur zones
            'overexposed': (255, 255, 0), # Yellow for overexposure
            'underexposed': (128, 0, 128), # Purple for underexposure
            'cropping': (255, 165, 0)    # Orange for cropping issues
        }
    
    def get_sample_images(self, count: int = 50) -> List[Path]:
        """
        Get a random sample of images from the input directory
        
        Args:
            count: Number of images to sample
            
        Returns:
            List of Path objects for selected images
        """
        logger.info(f"ðŸ“¸ Buscando imagens no diretÃ³rio: {self.input_dir}")
        
        # Get all image files
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        all_images = []
        
        for ext in image_extensions:
            all_images.extend(self.input_dir.glob(f'*{ext}'))
            all_images.extend(self.input_dir.glob(f'*{ext.upper()}'))
        
        logger.info(f"ðŸ“Š Total de imagens encontradas: {len(all_images)}")
        
        if len(all_images) == 0:
            logger.error("âŒ Nenhuma imagem encontrada no diretÃ³rio de entrada!")
            return []
        
        # Sample random images
        sample_count = min(count, len(all_images))
        sampled_images = random.sample(all_images, sample_count)
        
        logger.info(f"ðŸŽ¯ Selecionadas {sample_count} imagens para anÃ¡lise")
        return sampled_images
    
    def analyze_single_image(self, image_path: Path) -> Dict:
        """
        Perform comprehensive analysis on a single image
        
        Args:
            image_path: Path to the image file
            
        Returns:
            Dictionary with complete analysis results
        """
        logger.info(f"ðŸ” Analisando: {image_path.name}")
        
        try:
            # Load image
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"NÃ£o foi possÃ­vel carregar a imagem: {image_path}")
            
            # Initialize result structure
            result = {
                'filename': image_path.name,
                'filepath': str(image_path),
                'timestamp': datetime.now().isoformat(),
                'image_properties': {
                    'width': image.shape[1],
                    'height': image.shape[0],
                    'channels': image.shape[2],
                    'size_mb': image_path.stat().st_size / (1024 * 1024)
                },
                'analysis_results': {},
                'detections': {
                    'faces': [],
                    'persons': [],
                    'blur_zones': [],
                    'exposure_issues': []
                },
                'scores': {},
                'classification': {},
                'recommendations': []
            }
            
            # 1. Basic Quality Analysis
            logger.debug(f"  ðŸ“Š AnÃ¡lise de qualidade bÃ¡sica...")
            quality_result = self.quality_analyzer.analyze_single_image(str(image_path))
            result['analysis_results']['quality'] = quality_result
            
            # 2. Feature Extraction
            logger.debug(f"  ðŸ”§ ExtraÃ§Ã£o de caracterÃ­sticas...")
            features = self.feature_extractor.extract_features(str(image_path))
            result['analysis_results']['features'] = features
            
            # 3. Person Detection - Use advanced detector if available
            logger.debug(f"  ðŸ‘¤ DetecÃ§Ã£o de pessoas...")
            if self.person_detector:
                person_result = self._detect_persons_advanced(image_path)
            else:
                person_result = self._detect_persons_basic(image)
            result['analysis_results']['person_detection'] = person_result
            if person_result.get('persons'):
                for person in person_result['persons']:
                    result['detections']['persons'].append({
                        'bbox': person.get('bbox', []),
                        'confidence': person.get('confidence', 0),
                        'keypoints': person.get('keypoints', [])
                    })
            
            # 4. Face Detection - Basic CV2 cascade
            logger.debug(f"  ðŸ‘¥ DetecÃ§Ã£o de rostos...")
            face_result = self._detect_faces_basic(image)
            result['analysis_results']['face_analysis'] = face_result
            if face_result.get('faces'):
                for face in face_result['faces']:
                    result['detections']['faces'].append({
                        'bbox': face.get('location', []),
                        'confidence': face.get('confidence', 0),
                        'landmarks': face.get('landmarks', {}),
                        'quality_score': face.get('quality_score', 0)
                    })
            
            # 5. Exposure Analysis - Basic brightness analysis
            logger.debug(f"  ðŸ’¡ AnÃ¡lise de exposiÃ§Ã£o...")
            exposure_result = self._analyze_exposure_basic(image)
            result['analysis_results']['exposure'] = exposure_result
            
            # Detect exposure zones
            if exposure_result.get('overexposed_regions'):
                for region in exposure_result['overexposed_regions']:
                    result['detections']['exposure_issues'].append({
                        'type': 'overexposed',
                        'bbox': region.get('bbox', []),
                        'severity': region.get('severity', 'unknown')
                    })
            
            if exposure_result.get('underexposed_regions'):
                for region in exposure_result['underexposed_regions']:
                    result['detections']['exposure_issues'].append({
                        'type': 'underexposed',
                        'bbox': region.get('bbox', []),
                        'severity': region.get('severity', 'unknown')
                    })
            
            # 6. Cropping Analysis
            logger.debug(f"  âœ‚ï¸ AnÃ¡lise de enquadramento...")
            cropping_result = self._analyze_cropping_basic(
                image, result['analysis_results'].get('person_detection', {})
            )
            result['analysis_results']['cropping'] = cropping_result
            
            # 7. Unified Scoring - Use basic scoring
            logger.debug(f"  ðŸŽ¯ CÃ¡lculo de pontuaÃ§Ã£o unificada...")
            scoring_result = self._calculate_basic_score(result)
            result['scores'] = scoring_result.get('scores', {})
            result['classification'] = scoring_result.get('classification', {})
            
            # 8. Generate Recommendations
            result['recommendations'] = self._generate_recommendations(result)
            
            # Update statistics
            self._update_statistics(result)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Erro ao analisar {image_path.name}: {str(e)}")
            self.statistics['failed_analysis'] += 1
            return {
                'filename': image_path.name,
                'filepath': str(image_path),
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def create_visual_overlay(self, image_path: Path, analysis_result: Dict) -> np.ndarray:
        """
        Create visual overlay showing all detections on the image with enhanced visibility
        
        Args:
            image_path: Path to original image
            analysis_result: Analysis results from analyze_single_image
            
        Returns:
            Image with visual overlays
        """
        # Load original image
        image = cv2.imread(str(image_path))
        if image is None:
            return np.zeros((480, 640, 3), dtype=np.uint8)
        
        # Create overlay image
        overlay = image.copy()
        
        # Draw person detections with thick borders and labels
        for i, person in enumerate(analysis_result['detections'].get('persons', []), 1):
            bbox = person.get('bbox', [])
            if len(bbox) == 4:
                x, y, w, h = bbox
                # Draw thick red rectangle for person
                cv2.rectangle(overlay, (x, y), (x + w, y + h), self.colors['person'], 4)
                
                # Add semi-transparent background for text
                confidence = person.get('confidence', 0)
                label = f'PERSON {i} ({confidence:.2f})'
                (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
                cv2.rectangle(overlay, (x, y - text_h - 10), (x + text_w + 10, y), self.colors['person'], -1)
                
                # Add white text on colored background
                cv2.putText(overlay, label, (x + 5, y - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # Draw face detections with green borders
        for i, face in enumerate(analysis_result['detections'].get('faces', []), 1):
            bbox = face.get('bbox', [])
            if len(bbox) == 4:
                x, y, w, h = bbox
                # Draw thick green rectangle for face
                cv2.rectangle(overlay, (x, y), (x + w, y + h), self.colors['face'], 3)
                
                # Add label with quality score
                quality = face.get('quality_score', 0)
                confidence = face.get('confidence', 0)
                label = f'FACE {i} (Q:{quality:.2f})'
                (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                cv2.rectangle(overlay, (x, y - text_h - 8), (x + text_w + 8, y), self.colors['face'], -1)
                
                # Add white text
                cv2.putText(overlay, label, (x + 4, y - 4), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Draw exposure issues with yellow/cyan borders
        for i, issue in enumerate(analysis_result['detections'].get('exposure_issues', []), 1):
            bbox = issue.get('bbox', [])
            if len(bbox) == 4:
                x, y, w, h = bbox
                color = self.colors['overexposed'] if issue['type'] == 'overexposed' else (0, 255, 255)  # Cyan for underexposed
                cv2.rectangle(overlay, (x, y), (x + w, y + h), color, 3)
                
                label = f'{issue["type"].upper()} {i}'
                (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                cv2.rectangle(overlay, (x, y - text_h - 6), (x + text_w + 6, y), color, -1)
                cv2.putText(overlay, label, (x + 3, y - 3), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)
        
        # Add overall image quality indicators
        h, w = overlay.shape[:2]
        
        # Add blur indication if image is blurry
        blur_score = analysis_result['analysis_results'].get('quality', {}).get('sharpness_laplacian', 0)
        if blur_score < 50:  # Threshold for blur
            cv2.rectangle(overlay, (10, 10), (w - 10, 50), (0, 0, 255), 3)  # Red border for blur
            cv2.rectangle(overlay, (15, 15), (w - 15, 45), (0, 0, 255), -1)  # Fill
            blur_text = f'BLUR DETECTED (Score: {blur_score:.1f})'
            cv2.putText(overlay, blur_text, (25, 35), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # Add brightness indicator
        brightness = analysis_result['analysis_results'].get('quality', {}).get('brightness_mean', 0)
        if brightness < 50:  # Very dark
            cv2.rectangle(overlay, (10, h - 50), (w - 10, h - 10), (0, 0, 0), 3)
            cv2.rectangle(overlay, (15, h - 45), (w - 15, h - 15), (0, 0, 0), -1)
            bright_text = f'UNDEREXPOSED (Brightness: {brightness:.1f})'
            cv2.putText(overlay, bright_text, (25, h - 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        elif brightness > 200:  # Very bright
            cv2.rectangle(overlay, (10, h - 50), (w - 10, h - 10), (255, 255, 255), 3)
            cv2.rectangle(overlay, (15, h - 45), (w - 15, h - 15), (255, 255, 255), -1)
            bright_text = f'OVEREXPOSED (Brightness: {brightness:.1f})'
            cv2.putText(overlay, bright_text, (25, h - 25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)
        
        # Add final score indicator in top right
        final_score = analysis_result['scores'].get('final_score', 0)
        rating = analysis_result['classification'].get('rating', 'unknown').upper()
        
        score_text = f'SCORE: {final_score:.2f} ({rating})'
        (text_w, text_h), _ = cv2.getTextSize(score_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
        
        # Color based on score
        if final_score >= 0.8:
            score_color = (0, 255, 0)  # Green for excellent
        elif final_score >= 0.6:
            score_color = (0, 255, 255)  # Yellow for good
        elif final_score >= 0.4:
            score_color = (0, 165, 255)  # Orange for acceptable
        else:
            score_color = (0, 0, 255)  # Red for poor
        
        cv2.rectangle(overlay, (w - text_w - 20, 10), (w - 10, text_h + 20), score_color, -1)
        cv2.putText(overlay, score_text, (w - text_w - 15, text_h + 15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        return overlay
        rating = analysis_result['classification'].get('rating', 'unknown')
        h, w = overlay.shape[:2]
        
        # Score background
        cv2.rectangle(overlay, (w - 250, h - 80), (w - 10, h - 10), (0, 0, 0), -1)
        cv2.rectangle(overlay, (w - 250, h - 80), (w - 10, h - 10), (255, 255, 255), 2)
        
        # Score text
        cv2.putText(overlay, f'Score: {final_score:.1f}', (w - 240, h - 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(overlay, f'Rating: {rating.upper()}', (w - 240, h - 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return overlay
    
    def save_json_debug(self, analysis_result: Dict) -> None:
        """
        Save detailed analysis results as JSON for debugging
        
        Args:
            analysis_result: Complete analysis results
        """
        filename = Path(analysis_result['filename']).stem + '_analysis.json'
        json_path = self.json_dir / filename
        
        # Make sure all numpy arrays are converted to lists for JSON serialization
        def convert_numpy(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(item) for item in obj]
            return obj
        
        clean_result = convert_numpy(analysis_result)
        
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(clean_result, f, indent=2, ensure_ascii=False)
            logger.debug(f"âœ… JSON debug salvo: {json_path.name}")
        except Exception as e:
            logger.error(f"âŒ Erro ao salvar JSON {json_path.name}: {str(e)}")
    
    def create_comparison_image(self, image_path: Path, analysis_result: Dict) -> None:
        """
        Create side-by-side comparison of original vs annotated image with organized info panel
        
        Args:
            image_path: Path to original image
            analysis_result: Analysis results
        """
        try:
            # Load original image
            original = cv2.imread(str(image_path))
            if original is None:
                return
            
            # Create annotated version
            annotated = self.create_visual_overlay(image_path, analysis_result)
            
            # Convert BGR to RGB for matplotlib
            original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
            annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
            
            # Create figure with custom layout (2 images + info panel)
            fig = plt.figure(figsize=(24, 12))
            gs = GridSpec(2, 3, figure=fig, width_ratios=[1, 1, 0.7], height_ratios=[3, 1])
            
            # Original image (top left)
            ax1 = fig.add_subplot(gs[0, 0])
            ax1.imshow(original_rgb)
            ax1.set_title(f'Original - {image_path.name}', fontsize=14, fontweight='bold')
            ax1.axis('off')
            
            # Annotated image (top center)
            ax2 = fig.add_subplot(gs[0, 1])
            ax2.imshow(annotated_rgb)
            ax2.set_title('Detections & Analysis', fontsize=14, fontweight='bold')
            ax2.axis('off')
            
            # Information panel (top right)
            ax3 = fig.add_subplot(gs[0, 2])
            self._create_info_panel(ax3, analysis_result)
            ax3.axis('off')
            
            # Detection legend (bottom spanning all columns)
            ax4 = fig.add_subplot(gs[1, :])
            self._create_detection_legend(ax4, analysis_result)
            ax4.axis('off')
            
            plt.tight_layout()
            
            # Save comparison image
            output_filename = Path(analysis_result['filename']).stem + '_comparison.png'
            output_path = self.results_dir / output_filename
            plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')
            plt.close()

            logger.debug(f"âœ… ComparaÃ§Ã£o salva: {output_filename}")
            
        except Exception as e:
            logger.error(f"âŒ Erro ao criar comparaÃ§Ã£o para {image_path.name}: {str(e)}")
    
    def _create_analysis_summary_text(self, result: Dict) -> str:
        """Create formatted analysis summary text"""
        lines = [
            f"ANALYSIS SUMMARY - {result['filename']}",
            "=" * 50,
            f"Final Score: {result['scores'].get('final_score', 0):.2f}",
            f"Rating: {result['classification'].get('rating', 'unknown').upper()}",
            f"Blur Score: {result['analysis_results'].get('quality', {}).get('sharpness_laplacian', 0):.1f}",
            f"Brightness: {result['analysis_results'].get('quality', {}).get('brightness_mean', 0):.1f}",
            f"Faces Detected: {len(result['detections'].get('faces', []))}",
            f"Persons Detected: {len(result['detections'].get('persons', []))}",
            f"Exposure Issues: {len(result['detections'].get('exposure_issues', []))}",
            "",
            "RECOMMENDATIONS:",
            *[f"â€¢ {rec}" for rec in result.get('recommendations', [])]
        ]
        return "\\n".join(lines)
    
    def _generate_recommendations(self, result: Dict) -> List[str]:
        """Generate actionable recommendations based on analysis"""
        recommendations = []
        
        # Blur recommendations
        blur_score = result['analysis_results'].get('quality', {}).get('sharpness_laplacian', 0)
        if blur_score < 30:
            recommendations.append("Imagem muito desfocada - considere rejeitar")
        elif blur_score < 50:
            recommendations.append("Nitidez baixa - pode precisar de ajuste")
        
        # Exposure recommendations
        brightness = result['analysis_results'].get('quality', {}).get('brightness_mean', 0)
        if brightness < 50:
            recommendations.append("Imagem muito escura - aumentar exposiÃ§Ã£o")
        elif brightness > 200:
            recommendations.append("Imagem muito clara - reduzir exposiÃ§Ã£o")
        
        # Person detection recommendations
        person_count = len(result['detections'].get('persons', []))
        if person_count == 0:
            recommendations.append("Nenhuma pessoa detectada - verificar se Ã© adequada para o contexto")
        elif person_count > 3:
            recommendations.append("MÃºltiplas pessoas detectadas - verificar composiÃ§Ã£o")
        
        # Face quality recommendations
        faces = result['detections'].get('faces', [])
        if faces:
            low_quality_faces = [f for f in faces if f.get('quality_score', 0) < 0.5]
            if low_quality_faces:
                recommendations.append(f"{len(low_quality_faces)} rosto(s) com baixa qualidade detectado(s)")
        
        # Score-based recommendations
        final_score = result['scores'].get('final_score', 0)
        if final_score < 0.4:
            recommendations.append("PontuaÃ§Ã£o baixa - considere rejeitar ou aplicar correÃ§Ãµes significativas")
        elif final_score < 0.6:
            recommendations.append("PontuaÃ§Ã£o mÃ©dia - pode beneficiar de ajustes menores")
        elif final_score > 0.8:
            recommendations.append("Excelente qualidade - manter como estÃ¡")
        
        return recommendations if recommendations else ["Imagem estÃ¡ em boas condiÃ§Ãµes"]
    
    def _update_statistics(self, result: Dict) -> None:
        """Update global statistics with analysis result"""
        self.statistics['total_images'] += 1
        self.statistics['successful_analysis'] += 1
        
        # Blur detection
        blur_score = result['analysis_results'].get('quality', {}).get('sharpness_laplacian', 0)
        if blur_score < 50:
            self.statistics['blur_detected'] += 1
        
        # Face and person detection
        if result['detections'].get('faces'):
            self.statistics['faces_detected'] += 1
        if result['detections'].get('persons'):
            self.statistics['persons_detected'] += 1
        
        # Exposure issues
        if result['detections'].get('exposure_issues'):
            self.statistics['exposure_issues'] += 1
        
        # Cropping issues
        cropping_result = result['analysis_results'].get('cropping', {})
        if cropping_result.get('severity') in ['moderate', 'severe']:
            self.statistics['cropping_issues'] += 1
        
        # Score distribution
        rating = result['classification'].get('rating', 'unknown')
        if rating in self.statistics['score_distribution']:
            self.statistics['score_distribution'][rating] += 1
    
    def create_summary_report(self) -> None:
        """Create comprehensive summary report with statistics and visualizations"""
        logger.info("ðŸ“Š Criando relatÃ³rio resumo...")
        
        # Create summary statistics plot
        fig = plt.figure(figsize=(20, 16))
        gs = GridSpec(4, 3, figure=fig)
        
        # 1. Score Distribution Pie Chart
        ax1 = fig.add_subplot(gs[0, 0])
        score_dist = self.statistics['score_distribution']
        labels = [k.capitalize() for k, v in score_dist.items() if v > 0]
        sizes = [v for v in score_dist.values() if v > 0]
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc'][:len(labels)]
        
        if sizes:
            ax1.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)
            ax1.set_title('DistribuiÃ§Ã£o de PontuaÃ§Ãµes', fontweight='bold')
        
        # 2. Detection Statistics Bar Chart
        ax2 = fig.add_subplot(gs[0, 1])
        detection_categories = ['Blur Detectado', 'Rostos', 'Pessoas', 'Exp. Issues', 'Crop Issues']
        detection_counts = [
            self.statistics['blur_detected'],
            self.statistics['faces_detected'],
            self.statistics['persons_detected'],
            self.statistics['exposure_issues'],
            self.statistics['cropping_issues']
        ]
        
        bars = ax2.bar(detection_categories, detection_counts, color=['red', 'green', 'blue', 'orange', 'purple'])
        ax2.set_title('EstatÃ­sticas de DetecÃ§Ã£o', fontweight='bold')
        ax2.set_ylabel('NÃºmero de Imagens')
        plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
        
        # Add value labels on bars
        for bar, count in zip(bars, detection_counts):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{count}', ha='center', va='bottom')
        
        # 3. Processing Success Rate
        ax3 = fig.add_subplot(gs[0, 2])
        success_rate = (self.statistics['successful_analysis'] / max(self.statistics['total_images'], 1)) * 100
        failure_rate = 100 - success_rate
        
        ax3.pie([success_rate, failure_rate], labels=['Sucesso', 'Falha'], 
               autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'], startangle=90)
        ax3.set_title('Taxa de Sucesso da AnÃ¡lise', fontweight='bold')
        
        # 4. Score Distribution Histogram
        ax4 = fig.add_subplot(gs[1, :])
        scores = [r['scores'].get('final_score', 0) for r in self.analysis_results if 'scores' in r]
        if scores:
            ax4.hist(scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
            mean_score = float(np.mean(scores))
            median_score = float(np.median(scores))
            ax4.axvline(mean_score, color='red', linestyle='--', label=f'MÃ©dia: {mean_score:.2f}')
            ax4.axvline(median_score, color='green', linestyle='--', label=f'Mediana: {median_score:.2f}')
            ax4.set_xlabel('PontuaÃ§Ã£o Final')
            ax4.set_ylabel('FrequÃªncia')
            ax4.set_title('DistribuiÃ§Ã£o de PontuaÃ§Ãµes Finais', fontweight='bold')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        # 5. Quality Metrics Comparison
        ax5 = fig.add_subplot(gs[2, :])
        quality_metrics = ['Sharpness', 'Brightness', 'Contrast', 'Person Quality']
        
        # Collect quality data
        sharpness_scores = [r['analysis_results'].get('quality', {}).get('sharpness_laplacian', 0) 
                           for r in self.analysis_results if 'analysis_results' in r]
        brightness_scores = [r['analysis_results'].get('quality', {}).get('brightness_mean', 0) 
                            for r in self.analysis_results if 'analysis_results' in r]
        
        # Normalize scores for comparison (0-100 scale)
        norm_sharpness = [(min(s, 200) / 200) * 100 for s in sharpness_scores] if sharpness_scores else []
        norm_brightness = [(s / 255) * 100 for s in brightness_scores] if brightness_scores else []
        
        if norm_sharpness and norm_brightness:
            box_data = [norm_sharpness, norm_brightness]
            box_labels = ['Nitidez', 'Brilho']
            
            bp = ax5.boxplot(box_data, patch_artist=True)
            ax5.set_xticklabels(box_labels)
            colors = ['lightblue', 'lightgreen']
            for patch, color in zip(bp['boxes'], colors):
                patch.set_facecolor(color)
            
            ax5.set_ylabel('PontuaÃ§Ã£o Normalizada (0-100)')
            ax5.set_title('DistribuiÃ§Ã£o de MÃ©tricas de Qualidade', fontweight='bold')
            ax5.grid(True, alpha=0.3)
        
        # 6. Summary Statistics Text
        ax6 = fig.add_subplot(gs[3, :])
        ax6.axis('off')
        
        summary_text = f"""
RELATÃ“RIO DE ANÃLISE VISUAL - {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
{'='*80}

ESTATÃSTICAS GERAIS:
â€¢ Total de imagens analisadas: {self.statistics['total_images']}
â€¢ AnÃ¡lises bem-sucedidas: {self.statistics['successful_analysis']}
â€¢ AnÃ¡lises com falha: {self.statistics['failed_analysis']}
â€¢ Taxa de sucesso: {success_rate:.1f}%

DETECÃ‡Ã•ES:
â€¢ Imagens com blur detectado: {self.statistics['blur_detected']} ({(self.statistics['blur_detected']/max(self.statistics['total_images'],1)*100):.1f}%)
â€¢ Imagens com rostos detectados: {self.statistics['faces_detected']} ({(self.statistics['faces_detected']/max(self.statistics['total_images'],1)*100):.1f}%)
â€¢ Imagens com pessoas detectadas: {self.statistics['persons_detected']} ({(self.statistics['persons_detected']/max(self.statistics['total_images'],1)*100):.1f}%)
â€¢ Imagens com problemas de exposiÃ§Ã£o: {self.statistics['exposure_issues']} ({(self.statistics['exposure_issues']/max(self.statistics['total_images'],1)*100):.1f}%)
â€¢ Imagens com problemas de enquadramento: {self.statistics['cropping_issues']} ({(self.statistics['cropping_issues']/max(self.statistics['total_images'],1)*100):.1f}%)

QUALIDADE:
â€¢ PontuaÃ§Ã£o mÃ©dia: {np.mean(scores) if scores else 0:.2f}
â€¢ PontuaÃ§Ã£o mediana: {np.median(scores) if scores else 0:.2f}
â€¢ Melhor pontuaÃ§Ã£o: {max(scores) if scores else 0:.2f}
â€¢ Pior pontuaÃ§Ã£o: {min(scores) if scores else 0:.2f}

CLASSIFICAÃ‡ÃƒO:
â€¢ Excelente: {score_dist['excellent']} imagens
â€¢ Bom: {score_dist['good']} imagens  
â€¢ AceitÃ¡vel: {score_dist['acceptable']} imagens
â€¢ Ruim: {score_dist['poor']} imagens
â€¢ Rejeitado: {score_dist['rejected']} imagens
        """
        
        ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes, fontsize=10,
                fontfamily='monospace', verticalalignment='top',
                bbox=dict(boxstyle="round,pad=1", facecolor="lightgray", alpha=0.8))
        
        plt.tight_layout()
        
        # Save summary report
        summary_path = self.summary_dir / f'analysis_summary_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
        plt.savefig(summary_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"âœ… RelatÃ³rio resumo salvo: {summary_path}")
        
        # Also save statistics as JSON
        stats_json_path = self.summary_dir / f'statistics_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(stats_json_path, 'w', encoding='utf-8') as f:
            json.dump(self.statistics, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… EstatÃ­sticas salvas: {stats_json_path}")
    
    def run_analysis(self, num_images: int = 50) -> None:
        """
        Run complete analysis on sample images
        
        Args:
            num_images: Number of images to analyze
        """
        logger.info(f"ðŸš€ Iniciando anÃ¡lise visual de {num_images} imagens...")
        
        # Get sample images
        sample_images = self.get_sample_images(num_images)
        if not sample_images:
            logger.error("âŒ Nenhuma imagem disponÃ­vel para anÃ¡lise!")
            return
        
        logger.info(f"ðŸ“¸ Processando {len(sample_images)} imagens...")
        
        # Process each image
        for i, image_path in enumerate(sample_images, 1):
            logger.info(f"ðŸ“Š [{i}/{len(sample_images)}] Processando: {image_path.name}")
            
            # Analyze image
            result = self.analyze_single_image(image_path)
            self.analysis_results.append(result)
            
            # Skip visualization if analysis failed
            if 'error' in result:
                continue
            
            # Save JSON debug file
            self.save_json_debug(result)
            
            # Create visual comparison
            self.create_comparison_image(image_path, result)
            
            # Progress update
            if i % 10 == 0:
                logger.info(f"âœ… Progresso: {i}/{len(sample_images)} imagens processadas")
        
        # Create summary report
        self.create_summary_report()
        
        # Final report
        logger.info("ðŸŽ‰ AnÃ¡lise completa!")
        logger.info(f"ðŸ“ Resultados salvos em: {self.output_dir}")
        logger.info(f"ðŸ“Š Imagens analisadas: {self.statistics['successful_analysis']}")
        logger.info(f"ðŸ“‹ Arquivos JSON gerados: {len(os.listdir(self.json_dir))}")
        logger.info(f"ðŸ–¼ï¸ ComparaÃ§Ãµes visuais: {len(os.listdir(self.results_dir))}")
        logger.info(f"ðŸ“ˆ RelatÃ³rios: {len(os.listdir(self.summary_dir))}")

    def _detect_faces_basic(self, image: np.ndarray) -> Dict:
        """Basic face detection using CV2 Haar cascades"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # Simple face detection without cascades for demo
            # Using basic image analysis to simulate face detection
            h, w = gray.shape
            center_region = gray[h//4:3*h//4, w//4:3*w//4]
            avg_brightness = np.mean(center_region)
            
            # Mock face detection result
            faces = []
            if avg_brightness > 50:  # Simple heuristic for potential face region
                faces.append({
                    'location': [w//4, h//4, w//2, h//2],
                    'confidence': 0.8,
                    'quality_score': min(avg_brightness / 255.0, 1.0),
                    'landmarks': {}
                })
            
            return {
                'faces': faces,
                'face_count': len(faces),
                'analysis_method': 'basic_heuristic'
            }
        except Exception as e:
            logger.error(f"Erro na detecÃ§Ã£o de rostos: {str(e)}")
            return {'faces': [], 'face_count': 0, 'error': str(e)}
    
    def _detect_persons_basic(self, image: np.ndarray) -> Dict:
        """Basic person detection using simple heuristics"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            
            # Simple edge detection to simulate person detection
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges) / (h * w)
            
            persons = []
            if edge_density > 0.05:  # Threshold for significant edge content
                persons.append({
                    'bbox': [w//6, h//6, 2*w//3, 2*h//3],
                    'confidence': min(edge_density * 10, 1.0),
                    'keypoints': []
                })
            
            return {
                'persons': persons,
                'person_count': len(persons),
                'analysis_method': 'edge_density_heuristic'
            }
        except Exception as e:
            logger.error(f"Erro na detecÃ§Ã£o de pessoas: {str(e)}")
            return {'persons': [], 'person_count': 0, 'error': str(e)}
    
    def _analyze_exposure_basic(self, image: np.ndarray) -> Dict:
        """Basic exposure analysis"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            h, w = gray.shape
            
            # Calculate exposure metrics
            mean_brightness = np.mean(gray)
            std_brightness = np.std(gray)
            
            # Detect overexposed regions (very bright)
            overexposed_mask = gray > 240
            overexposed_regions = []
            if np.sum(overexposed_mask) > (h * w * 0.1):  # More than 10% overexposed
                # Find bounding box of overexposed region
                coords = np.column_stack(np.where(overexposed_mask))
                if len(coords) > 0:
                    y_min, x_min = coords.min(axis=0)
                    y_max, x_max = coords.max(axis=0)
                    overexposed_regions.append({
                        'bbox': [x_min, y_min, x_max-x_min, y_max-y_min],
                        'severity': 'high' if np.sum(overexposed_mask) > (h * w * 0.3) else 'moderate'
                    })
            
            # Detect underexposed regions (very dark)
            underexposed_mask = gray < 30
            underexposed_regions = []
            if np.sum(underexposed_mask) > (h * w * 0.1):  # More than 10% underexposed
                coords = np.column_stack(np.where(underexposed_mask))
                if len(coords) > 0:
                    y_min, x_min = coords.min(axis=0)
                    y_max, x_max = coords.max(axis=0)
                    underexposed_regions.append({
                        'bbox': [x_min, y_min, x_max-x_min, y_max-y_min],
                        'severity': 'high' if np.sum(underexposed_mask) > (h * w * 0.3) else 'moderate'
                    })
            
            return {
                'mean_brightness': float(mean_brightness),
                'brightness_std': float(std_brightness),
                'overexposed_regions': overexposed_regions,
                'underexposed_regions': underexposed_regions,
                'exposure_quality': 'good' if 80 <= mean_brightness <= 180 else 'poor'
            }
        except Exception as e:
            logger.error(f"Erro na anÃ¡lise de exposiÃ§Ã£o: {str(e)}")
            return {'error': str(e)}
    
    def _analyze_cropping_basic(self, image: np.ndarray, person_analysis: Dict) -> Dict:
        """Basic cropping analysis"""
        try:
            h, w = image.shape[:2]
            
            # Simple cropping assessment based on person detection
            persons = person_analysis.get('persons', [])
            
            if not persons:
                return {
                    'severity': 'unknown',
                    'cropping_score': 0.5,
                    'recommendations': ['Nenhuma pessoa detectada para anÃ¡lise de enquadramento']
                }
            
            # Check if person bbox is well-positioned
            person = persons[0]  # Take first person
            bbox = person.get('bbox', [])
            
            if len(bbox) == 4:
                x, y, pw, ph = bbox
                
                # Calculate margins
                left_margin = x / w
                right_margin = (w - x - pw) / w
                top_margin = y / h
                bottom_margin = (h - y - ph) / h
                
                # Simple cropping score based on margins
                min_margin = min(left_margin, right_margin, top_margin, bottom_margin)
                
                if min_margin < 0.05:
                    severity = 'severe'
                    score = 0.2
                elif min_margin < 0.15:
                    severity = 'moderate'
                    score = 0.5
                else:
                    severity = 'good'
                    score = 0.8
                
                return {
                    'severity': severity,
                    'cropping_score': score,
                    'margins': {
                        'left': left_margin,
                        'right': right_margin,
                        'top': top_margin,
                        'bottom': bottom_margin
                    },
                    'recommendations': [
                        f'Enquadramento {severity}',
                        f'Margem mÃ­nima: {min_margin:.2f}'
                    ]
                }
            
            return {
                'severity': 'unknown',
                'cropping_score': 0.5,
                'recommendations': ['NÃ£o foi possÃ­vel analisar enquadramento']
            }
        except Exception as e:
            logger.error(f"Erro na anÃ¡lise de enquadramento: {str(e)}")  
            return {'error': str(e)}
    
    def _calculate_basic_score(self, analysis_result: Dict) -> Dict:
        """Calculate basic unified score"""
        try:
            # Get quality metrics
            quality = analysis_result['analysis_results'].get('quality', {})
            blur_score = quality.get('blur_score', 0)
            brightness = quality.get('mean_brightness', 0)
            
            # Get detection counts
            face_count = len(analysis_result['detections'].get('faces', []))
            person_count = len(analysis_result['detections'].get('persons', []))
            
            # Calculate component scores (0-1 scale)
            # Blur component (40% weight)
            blur_component = min(blur_score / 100.0, 1.0) if blur_score > 0 else 0
            
            # Brightness component (20% weight)  
            brightness_component = 1.0 if 80 <= brightness <= 180 else max(0, 1.0 - abs(brightness - 130) / 130)
            
            # Person detection component (30% weight)
            person_component = min(person_count / 2.0, 1.0)  # Optimal 1-2 persons
            
            # Face detection component (10% weight)
            face_component = min(face_count / 3.0, 1.0)  # Up to 3 faces is good
            
            # Calculate weighted final score
            final_score = (
                blur_component * 0.4 +
                brightness_component * 0.2 +
                person_component * 0.3 +
                face_component * 0.1
            )
            
            # Classify rating
            if final_score >= 0.85:
                rating = 'excellent'
            elif final_score >= 0.70:
                rating = 'good'
            elif final_score >= 0.60:
                rating = 'acceptable'
            elif final_score >= 0.40:
                rating = 'poor'
            else:
                rating = 'rejected'
            
            return {
                'scores': {
                    'final_score': final_score,
                    'blur_component': blur_component,
                    'brightness_component': brightness_component,
                    'person_component': person_component,
                    'face_component': face_component
                },
                'classification': {
                    'rating': rating,
                    'confidence': min(final_score * 1.2, 1.0)
                }
            }
        except Exception as e:
            logger.error(f"Erro no cÃ¡lculo de pontuaÃ§Ã£o: {str(e)}")
            return {
                'scores': {'final_score': 0.0},
                'classification': {'rating': 'error'}
            }
    
    def _create_info_panel(self, ax, analysis_result: Dict) -> None:
        """
        Create organized information panel with analysis results
        
        Args:
            ax: Matplotlib axis for the info panel
            analysis_result: Analysis results dictionary
        """
        try:
            # Extract key information
            scores = analysis_result.get('scores', {})
            quality = analysis_result.get('analysis_results', {}).get('quality', {})
            detections = analysis_result.get('detections', {})
            classification = analysis_result.get('classification', {})
            
            # Create structured text information
            info_lines = [
                "ðŸ“Š ANALYSIS RESULTS",
                "=" * 25,
                "",
                "ðŸŽ¯ SCORES:",
                f"  Final Score: {scores.get('final_score', 0):.2f}",
                f"  Rating: {classification.get('rating', 'unknown').upper()}",
                f"  Quality: {classification.get('quality_level', 'unknown')}",
                "",
                "ðŸ” TECHNICAL METRICS:",
                f"  Blur Score: {quality.get('sharpness_laplacian', 0):.1f}",
                f"  Brightness: {quality.get('brightness_mean', 0):.1f}",
                f"  Contrast: {quality.get('contrast_rms', 0):.1f}",
                f"  Edge Density: {quality.get('edge_density', 0):.3f}",
                "",
                "ðŸ‘¥ DETECTIONS:",
                f"  Faces: {len(detections.get('faces', []))}",
                f"  Persons: {len(detections.get('persons', []))}",
                f"  Exposure Issues: {len(detections.get('exposure_issues', []))}",
                "",
                "ðŸ’¡ RECOMMENDATIONS:",
            ]
            
            # Add recommendations (max 5 to fit)
            recommendations = analysis_result.get('recommendations', [])[:5]
            for i, rec in enumerate(recommendations, 1):
                # Wrap long recommendations
                if len(rec) > 35:
                    rec = rec[:32] + "..."
                info_lines.append(f"  {i}. {rec}")
            
            # Display text on axis
            text_content = "\n".join(info_lines)
            ax.text(0.05, 0.95, text_content, transform=ax.transAxes, 
                   fontsize=10, fontfamily='monospace', verticalalignment='top',
                   bbox=dict(boxstyle="round,pad=0.8", facecolor="lightblue", alpha=0.8))
            
        except Exception as e:
            logger.error(f"Erro ao criar painel de informaÃ§Ãµes: {e}")
            ax.text(0.05, 0.5, f"Erro ao exibir informaÃ§Ãµes:\n{str(e)}", 
                   transform=ax.transAxes, fontsize=10, color='red')
    
    def _create_detection_legend(self, ax, analysis_result: Dict) -> None:
        """
        Create detection legend showing what each color/box represents
        
        Args:
            ax: Matplotlib axis for the legend
            analysis_result: Analysis results dictionary
        """
        try:
            detections = analysis_result.get('detections', {})
            
            # Create legend items based on actual detections
            legend_items = []
            
            # Add legend items for detected elements
            if detections.get('faces'):
                legend_items.append(("ðŸŸ¢ Green Box", "Face Detection", f"({len(detections['faces'])} detected)"))
            
            if detections.get('persons'):
                legend_items.append(("ðŸ”´ Red Box", "Person Detection", f"({len(detections['persons'])} detected)"))
            
            if detections.get('exposure_issues'):
                legend_items.append(("ðŸŸ¡ Yellow Box", "Exposure Issues", f"({len(detections['exposure_issues'])} detected)"))
            
            # Add general detection info
            if detections.get('blur_zones'):
                legend_items.append(("ðŸ”µ Blue Box", "Blur Zones", f"({len(detections['blur_zones'])} detected)"))
            
            # Create legend layout
            if legend_items:
                legend_text = "ðŸŽ¯ DETECTION LEGEND:\n"
                for color, detection_type, count in legend_items:
                    legend_text += f"  {color}: {detection_type} {count}\n"
                
                # Add detection summary
                total_detections = sum([
                    len(detections.get('faces', [])),
                    len(detections.get('persons', [])),
                    len(detections.get('exposure_issues', [])),
                    len(detections.get('blur_zones', []))
                ])
                
                legend_text += f"\nðŸ“ˆ Total Detections: {total_detections}"
                
                # Add confidence summary if available
                face_confidences = [f.get('confidence', 0) for f in detections.get('faces', [])]
                if face_confidences:
                    avg_confidence = sum(face_confidences) / len(face_confidences)
                    legend_text += f"\nðŸŽ¯ Avg Face Confidence: {avg_confidence:.2f}"
                
            else:
                legend_text = "ðŸŽ¯ DETECTION LEGEND:\n  No detections found in this image"
            
            # Display legend
            ax.text(0.05, 0.8, legend_text, transform=ax.transAxes, 
                   fontsize=11, fontfamily='monospace', verticalalignment='top',
                   bbox=dict(boxstyle="round,pad=0.5", facecolor="lightyellow", alpha=0.8))
            
        except Exception as e:
            logger.error(f"Erro ao criar legenda: {e}")
            ax.text(0.05, 0.5, f"Erro na legenda: {str(e)}", 
                   transform=ax.transAxes, fontsize=10, color='red')

    def _detect_persons_advanced(self, image_path: Path) -> Dict:
        """Advanced person detection using PersonDetector module"""
        try:
            if not self.person_detector:
                return self._detect_persons_basic(cv2.imread(str(image_path)))
            
            # Use the real PersonDetector
            result = self.person_detector.detect_persons(str(image_path))
            
            persons = []
            if result.get('detections'):
                for detection in result['detections']:
                    persons.append({
                        'bbox': detection.get('bbox', []),
                        'confidence': detection.get('confidence', 0),
                        'keypoints': detection.get('keypoints', []),
                        'body_parts': detection.get('body_parts', {})
                    })
            
            return {
                'persons': persons,
                'person_count': len(persons),
                'analysis_method': 'mediapipe_advanced',
                'total_detections': result.get('person_count', 0),
                'processing_info': result.get('processing_info', {})
            }
            
        except Exception as e:
            logger.error(f"Erro na detecÃ§Ã£o avanÃ§ada de pessoas: {str(e)}")
            # Fallback to basic detection
            return self._detect_persons_basic(cv2.imread(str(image_path)))
    
    def _detect_faces_advanced(self, image_path: Path) -> Dict:
        """Advanced face detection using FaceRecognitionSystem"""
        try:
            if not self.face_recognition:
                return self._detect_faces_basic(cv2.imread(str(image_path)))
            
            # Use the real FaceRecognitionSystem
            result = self.face_recognition.detect_and_analyze_faces(str(image_path))
            
            faces = []
            if result.get('faces'):
                for face in result['faces']:
                    faces.append({
                        'location': face.get('location', []),
                        'bbox': face.get('bbox', []),
                        'confidence': face.get('confidence', 0),
                        'landmarks': face.get('landmarks', {}),
                        'quality_score': face.get('quality_analysis', {}).get('overall_quality', 0),
                        'encoding': face.get('encoding', [])
                    })
            
            return {
                'faces': faces,
                'face_count': len(faces),
                'analysis_method': 'face_recognition_advanced',
                'processing_info': result.get('processing_info', {})
            }
            
        except Exception as e:
            logger.error(f"Erro na detecÃ§Ã£o avanÃ§ada de rostos: {str(e)}")
            # Fallback to basic detection
            return self._detect_faces_basic(cv2.imread(str(image_path)))
